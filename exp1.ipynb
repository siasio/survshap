{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SurvSHAP(t): Time-Dependent Explanations Of Machine Learning Survival Models\n",
    "### M. KrzyziÅ„ski, M. Spytek, H. Baniecki, P. Biecek\n",
    "## Experiment 1: Evaluating explanations on synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data and models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/exp1_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.util import Surv\n",
    "X = data.iloc[:, :5]\n",
    "y = Surv.from_dataframe(\"event\", \"time\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.622925665249838"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "cph = CoxPHSurvivalAnalysis()\n",
    "cph.fit(X, y)\n",
    "cph.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [6], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msksurv\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mensemble\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RandomSurvivalForest\n\u001B[0;32m      2\u001B[0m rsf \u001B[38;5;241m=\u001B[39m RandomSurvivalForest(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, min_samples_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, min_samples_leaf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, max_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, max_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mrsf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m rsf\u001B[38;5;241m.\u001B[39mscore(X, y)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sksurv\\ensemble\\forest.py:144\u001B[0m, in \u001B[0;36m_BaseSurvivalForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    134\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    135\u001B[0m                               random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    136\u001B[0m          \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)]\n\u001B[0;32m    138\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 144\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43my_numeric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevent_times_\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1051\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[0;32m   1049\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1051\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m   1054\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1055\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[0;32m   1056\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[0;32m   1057\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:864\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    863\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 864\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    865\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:782\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    780\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    781\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 782\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    785\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    786\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    787\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:263\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    262\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 263\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    264\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:263\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    262\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 263\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    264\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    187\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 189\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    191\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sksurv\\tree\\tree.py:253\u001B[0m, in \u001B[0;36mSurvivalTree.fit\u001B[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001B[0m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    243\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    244\u001B[0m         splitter,\n\u001B[0;32m    245\u001B[0m         params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin_samples_split\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    250\u001B[0m         \u001B[38;5;241m0.0\u001B[39m,  \u001B[38;5;66;03m# min_impurity_decrease\u001B[39;00m\n\u001B[0;32m    251\u001B[0m     )\n\u001B[1;32m--> 253\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_numeric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "rsf = RandomSurvivalForest(random_state=42, n_estimators=100, min_samples_split=8, min_samples_leaf=4, max_features=3, max_samples=0.8)\n",
    "rsf.fit(X, y)\n",
    "rsf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating performance of models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import brier_score, integrated_brier_score\n",
    "# mask created to enable for calculating Brier score\n",
    "mask = (y[\"time\"] < y[y[\"event\"]==1][\"time\"].max()) & (y[\"time\"] > y[y[\"event\"]==1][\"time\"].min())\n",
    "times = np.percentile(y[mask][\"time\"], np.linspace(0.1, 99.9, 101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survs_rsf = rsf.predict_survival_function(X[mask])\n",
    "survs_cph = cph.predict_survival_function(X[mask])\n",
    "preds_rsf = [fn(times) for fn in survs_rsf]\n",
    "preds_cph = [fn(times) for fn in survs_cph]\n",
    "brier_rsf = brier_score(y, y[mask], preds_rsf, times)\n",
    "brier_cph = brier_score(y, y[mask], preds_cph, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame({\"time\": times, \"brier_score\":  brier_rsf[1], \"label\": \"RSF\"}),\n",
    "            pd.DataFrame({\"time\": times, \"brier_score\":  brier_cph[1], \"label\": \"CPH\"})]).to_csv(\"results/exp1_model_brier_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_brier_score(y, y[mask], preds_rsf, times), integrated_brier_score(y, y[mask], preds_cph, times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from survshap import SurvivalModelExplainer, PredictSurvSHAP, ModelSurvSHAP\n",
    "rsf_exp = SurvivalModelExplainer(rsf, X, y)\n",
    "cph_exp = SurvivalModelExplainer(cph, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_survshap_global_rsf = ModelSurvSHAP(random_state=42)\n",
    "exp1_survshap_global_rsf.fit(rsf_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"pickles/exp1_survshap_global_rsf\", \"wb\") as file:\n",
    "    pickle.dump(exp1_survshap_global_rsf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_survshap_global_cph = ModelSurvSHAP(random_state=42)\n",
    "exp1_survshap_global_cph.fit(cph_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pickles/exp1_survshap_global_cph\", \"wb\") as file:\n",
    "    pickle.dump(exp1_survshap_global_cph, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating ground-truth SurvSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "def shap_kernel(\n",
    "    explainer, new_observation, function_type, timestamps, baseline_f, simplified_inputs, kernel_weights, n\n",
    "):\n",
    "    data = generate_data(n)\n",
    "\n",
    "    shap_values, r2 = calculate_shap_values(\n",
    "        explainer.model,\n",
    "        function_type,\n",
    "        baseline_f,\n",
    "        data,\n",
    "        simplified_inputs,\n",
    "        kernel_weights,\n",
    "        new_observation,\n",
    "        timestamps,\n",
    "    )\n",
    "    result_shap = pd.DataFrame(\n",
    "        shap_values, columns=[\" = \".join([\"t\", str(time)]) for time in timestamps]\n",
    "    )\n",
    "\n",
    "    return result_shap, r2\n",
    "\n",
    "\n",
    "def generate_shap_kernel_weights(simplified_inputs, num_variables):\n",
    "    weights = []\n",
    "    for coalition_vector in simplified_inputs:\n",
    "        num_available_variables = np.count_nonzero(coalition_vector)\n",
    "        if num_available_variables == 0 or num_available_variables == num_variables:\n",
    "            weights.append(1e9)\n",
    "        else:\n",
    "            weights.append(\n",
    "                (num_variables - 1)\n",
    "                / (\n",
    "                    math.comb(num_variables, num_available_variables)\n",
    "                    * num_available_variables\n",
    "                    * (num_variables - num_available_variables)\n",
    "                )\n",
    "            )\n",
    "    return weights\n",
    "\n",
    "\n",
    "def make_prediction_for_simplified_input(\n",
    "    model, function_type, data, simplified_inputs, new_observation, timestamps\n",
    "):\n",
    "    preds = np.zeros((len(simplified_inputs), len(timestamps)))\n",
    "    for i, mask in enumerate(simplified_inputs):\n",
    "        X_tmp = pd.DataFrame(\n",
    "            np.where(mask, new_observation, data), columns=data.columns\n",
    "        )\n",
    "        preds[\n",
    "            i,\n",
    "        ] = calculate_mean_function(model, function_type, X_tmp, timestamps)\n",
    "    return preds\n",
    "\n",
    "def calculate_mean_function(model, function_type, data, timestamps):\n",
    "    if function_type == \"sf\":\n",
    "        all_functions = model.predict_survival_function(data)\n",
    "    elif function_type == \"chf\":\n",
    "        all_functions = model.predict_cumulative_hazard_function(data)\n",
    "    all_function_vals = [f(timestamps) for f in all_functions]\n",
    "    return np.mean(all_function_vals, axis=0)\n",
    "\n",
    "\n",
    "def calculate_shap_values(\n",
    "    model,\n",
    "    function_type,\n",
    "    avg_function,\n",
    "    data,\n",
    "    simplified_inputs,\n",
    "    shap_kernel_weights,\n",
    "    new_observation,\n",
    "    timestamps,\n",
    "):\n",
    "    W = np.diag(shap_kernel_weights)\n",
    "    X = np.array(simplified_inputs)\n",
    "    R = np.linalg.inv(X.T @ W @ X) @ (X.T @ W)\n",
    "    y = (\n",
    "        make_prediction_for_simplified_input(\n",
    "            model, function_type, data, simplified_inputs, new_observation, timestamps\n",
    "        )\n",
    "        - avg_function\n",
    "    )\n",
    "    shap_values = R @ y\n",
    "    y_pred = X @ shap_values\n",
    "    r2 = [None] * y.shape[1]\n",
    "    for i in range(y.shape[1]):\n",
    "        r2[i] = r2_score(y[:, i], y_pred[:, i], sample_weight=shap_kernel_weights)\n",
    "    return shap_values, r2\n",
    "\n",
    "def generate_data(n):\n",
    "    x1 = np.random.binomial(1, 0.5, n)\n",
    "    x2 = np.random.binomial(1, 0.5, n)\n",
    "    x3 = np.random.normal(10, 2, n)\n",
    "    x4 = np.random.normal(20, 4, n)\n",
    "    x5 = np.random.normal(0, 1, n)\n",
    "    return  pd.DataFrame({\"x1\": x1, \"x2\": x2, \"x3\": x3, \"x4\": x4, \"x5\": x5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_functions_rsf = rsf.predict_survival_function(X)\n",
    "all_functions_vals = [f.y for f in all_functions_rsf]\n",
    "timestamps = all_functions_rsf[0].x\n",
    "baseline_f = np.mean(all_functions_vals, axis=0)\n",
    "simplified_inputs = [list(z) for z in itertools.product(range(2), repeat=5)]\n",
    "kernel_weights = generate_shap_kernel_weights(simplified_inputs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y[\"event\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_groundtruth = pd.DataFrame()\n",
    "to_calculate = list(X_test.index)\n",
    "for i in tqdm(to_calculate):\n",
    "    shap_gt = shap_kernel(\n",
    "        rsf_exp, X.iloc[[i]], \"sf\", timestamps, baseline_f, simplified_inputs, kernel_weights, 10000\n",
    "    )\n",
    "    shap_gt[0].insert(0, \"index\", i)\n",
    "    shap_groundtruth = pd.concat([shap_groundtruth, shap_gt[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_groundtruth.to_csv(\"results/exp1_shap_groundtruth_rsf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "all_functions_cph = cph.predict_survival_function(X)\n",
    "all_functions_vals = [f.y for f in all_functions_cph]\n",
    "timestamps = all_functions_cph[0].x\n",
    "baseline_f = np.mean(all_functions_vals, axis=0)\n",
    "simplified_inputs = [list(z) for z in itertools.product(range(2), repeat=5)]\n",
    "kernel_weights = generate_shap_kernel_weights(simplified_inputs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_groundtruth_cph = pd.DataFrame()\n",
    "to_calculate = list(X_test.index)\n",
    "for i in tqdm(to_calculate):\n",
    "    shap_gt = shap_kernel(\n",
    "        cph_exp, X.iloc[[i]], \"sf\", timestamps, baseline_f, simplified_inputs, kernel_weights, 10000\n",
    "    )\n",
    "    shap_gt[0].insert(0, \"index\", i)\n",
    "    shap_groundtruth_cph = pd.concat([shap_groundtruth_cph, shap_gt[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_groundtruth_cph.to_csv(\"results/exp1_shap_groundtruth_cph.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_grountruth_rsf = pd.read_csv(\"results/exp1_shap_groundtruth_rsf.csv\")\n",
    "shap_grountruth_cph = pd.read_csv(\"results/exp1_shap_groundtruth_cph.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pickles/exp1_survshap_global_rsf\", \"rb\") as file:\n",
    "    exp1_survshap_global_rsf = pickle.load(file)\n",
    "with open(\"pickles/exp1_survshap_global_cph\", \"rb\") as file:\n",
    "    exp1_survshap_global_cph = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SurvSHAP(t) values plot example\n",
    "\n",
    "example_rsf = deepcopy(exp1_survshap_global_rsf.individual_explanations[690])\n",
    "example_cph = deepcopy(exp1_survshap_global_cph.individual_explanations[690])\n",
    "\n",
    "melted_example_rsf = pd.melt(example_rsf.result, id_vars=\"variable_name\", value_vars=example_rsf.result.columns[6:])\n",
    "melted_example_rsf[\"variable\"] = melted_example_rsf[\"variable\"].str[4:].astype(float)\n",
    "melted_example_rsf.to_csv(\"results/exp1_example_rsf.csv\", index=False)\n",
    "melted_example_cph = pd.melt(example_cph.result, id_vars=\"variable_name\", value_vars=example_cph.result.columns[6:])\n",
    "melted_example_cph[\"variable\"] = melted_example_cph[\"variable\"].str[4:].astype(float)\n",
    "melted_example_cph.to_csv(\"results/exp1_example_cph.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized SurvSHAP(t) values plot example\n",
    "\n",
    "example_rsf.result.iloc[:, 5:] = np.nan_to_num(\n",
    "            example_rsf.result[example_rsf.result[\"B\"] == 0].iloc[:, 5:]\n",
    "            / example_rsf.result[example_rsf.result[\"B\"] == 0].iloc[:, 5:].abs().sum())\n",
    "\n",
    "example_cph.result.iloc[:, 5:] = np.nan_to_num(\n",
    "            example_cph.result[example_cph.result[\"B\"] == 0].iloc[:, 5:]\n",
    "            / example_cph.result[example_cph.result[\"B\"] == 0].iloc[:, 5:].abs().sum())\n",
    "\n",
    "melted_example_norm_rsf = pd.melt(example_rsf.result, id_vars=\"variable_name\", value_vars=example_rsf.result.columns[6:])\n",
    "melted_example_norm_rsf[\"variable\"] = melted_example_norm_rsf[\"variable\"].str[4:].astype(float)\n",
    "melted_example_norm_rsf.to_csv(\"results/exp1_example_norm_rsf.csv\", index=False)\n",
    "\n",
    "melted_example_norm_cph = pd.melt(example_cph.result, id_vars=\"variable_name\", value_vars=example_cph.result.columns[6:])\n",
    "melted_example_norm_cph[\"variable\"] = melted_example_norm_cph[\"variable\"].str[4:].astype(float)\n",
    "melted_example_norm_cph.to_csv(\"results/exp1_example_norm_cph.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changing Signs Proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_signs_rsf = np.sign(exp1_survshap_global_rsf.full_result.iloc[:, 6:].values)\n",
    "timestamps_rsf = exp1_survshap_global_rsf.timestamps\n",
    "\n",
    "shap_signs_cph = np.sign(exp1_survshap_global_cph.full_result.iloc[:, 6:].values)\n",
    "timestamps_cph = exp1_survshap_global_cph.timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index, end_index = np.where((timestamps_rsf >= np.percentile(timestamps_rsf, 10)) & (timestamps_rsf <= np.percentile(timestamps_rsf, 90)))[0][[0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_ranges = []\n",
    "for row in shap_signs_rsf:\n",
    "    sign_ranges_row = []\n",
    "    last_sign = row[start_index]\n",
    "    start_time_sign_sequence = timestamps_rsf[start_index]\n",
    "    for i in range(start_index, end_index+1):\n",
    "        if row[i] != last_sign and row[i] != 0:\n",
    "            sign_ranges_row.append(last_sign*(timestamps_rsf[i-1] - start_time_sign_sequence))\n",
    "            start_time_sign_sequence = timestamps_rsf[i-1]\n",
    "        if row[i] != 0:\n",
    "            last_sign = row[i] \n",
    "    sign_ranges_row.append(last_sign*(timestamps_rsf[i] - start_time_sign_sequence))\n",
    "    sign_ranges.append(sign_ranges_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_range = [sum(sign_seq_range for sign_seq_range in sign_ranges_row if sign_seq_range < 0) for sign_ranges_row in sign_ranges]\n",
    "positive_range = [sum(sign_seq_range for sign_seq_range in sign_ranges_row if sign_seq_range > 0) for sign_ranges_row in sign_ranges]\n",
    "timestamps_range = timestamps_rsf[end_index] - timestamps_rsf[start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_signs_005 = (np.abs(np.array(negative_range)) >= 0.05 * timestamps_range) & (np.array(positive_range) >= 0.05 * timestamps_range)\n",
    "changed_signs_01 = (np.abs(np.array(negative_range)) >= 0.1 * timestamps_range) & (np.array(positive_range) >= 0.1 * timestamps_range)\n",
    "changed_signs_02 = (np.abs(np.array(negative_range)) >= 0.2 * timestamps_range) & (np.array(positive_range) >= 0.2 * timestamps_range)\n",
    "csp_rsf = pd.DataFrame({\"variable_name\": exp1_survshap_global_rsf.full_result.variable_name, \n",
    "                                \"variable_value\": exp1_survshap_global_rsf.full_result.variable_value, \n",
    "                                \"index\": exp1_survshap_global_rsf.full_result.index, \n",
    "                                \"changed_signs_0.05\": changed_signs_005,\n",
    "                                \"changed_signs_0.1\": changed_signs_01,\n",
    "                                \"changed_signs_0.2\": changed_signs_02})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index, end_index = np.where((timestamps_cph >= np.percentile(timestamps_rsf, 10)) & (timestamps_cph <= np.percentile(timestamps_rsf, 90)))[0][[0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_ranges = []\n",
    "for row in shap_signs_cph:\n",
    "    sign_ranges_row = []\n",
    "    last_sign = row[start_index]\n",
    "    start_time_sign_sequence = timestamps_cph[start_index]\n",
    "    for i in range(start_index, end_index):\n",
    "        if row[i] != last_sign and row[i] != 0:\n",
    "            sign_ranges_row.append(last_sign*(timestamps_cph[i-1] - start_time_sign_sequence))\n",
    "            start_time_sign_sequence = timestamps_cph[i-1]\n",
    "        if row[i] != 0:\n",
    "            last_sign = row[i] \n",
    "    sign_ranges_row.append(last_sign*(timestamps_cph[i] - start_time_sign_sequence))\n",
    "    sign_ranges.append(sign_ranges_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_range = [sum(sign_seq_range for sign_seq_range in sign_ranges_row if sign_seq_range < 0) for sign_ranges_row in sign_ranges]\n",
    "positive_range = [sum(sign_seq_range for sign_seq_range in sign_ranges_row if sign_seq_range > 0) for sign_ranges_row in sign_ranges]\n",
    "timestamps_range = timestamps_cph[-1] - timestamps_cph[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_signs_005 = (np.abs(np.array(negative_range)) >= 0.05 * timestamps_range) & (np.array(positive_range) >= 0.05 * timestamps_range)\n",
    "changed_signs_01 = (np.abs(np.array(negative_range)) >= 0.1 * timestamps_range) & (np.array(positive_range) >= 0.1 * timestamps_range)\n",
    "changed_signs_02 = (np.abs(np.array(negative_range)) >= 0.2 * timestamps_range) & (np.array(positive_range) >= 0.2 * timestamps_range)\n",
    "csp_cph = pd.DataFrame({\"variable_name\": exp1_survshap_global_cph.full_result.variable_name, \n",
    "                                \"variable_value\": exp1_survshap_global_cph.full_result.variable_value, \n",
    "                                \"index\": exp1_survshap_global_cph.full_result.index, \n",
    "                                \"changed_signs_0.05\": changed_signs_005,\n",
    "                                \"changed_signs_0.1\": changed_signs_01,\n",
    "                                \"changed_signs_0.2\": changed_signs_02})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csp_rsf.groupby(\"variable_name\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csp_cph.groupby(\"variable_name\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Local accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_accuracy_from_shap_explanations(all_explanation, label, last_index=None):\n",
    "    if last_index is None:\n",
    "        last_index=len(all_explanation.timestamps)\n",
    "    diffs = []\n",
    "    preds = []\n",
    "    for explanation in all_explanation.individual_explanations:\n",
    "        preds.append(explanation.predicted_function[:last_index])\n",
    "        diffs.append(explanation.predicted_function[:last_index] - explanation.baseline_function[:last_index] - np.array(explanation.result.iloc[:, 6:].sum(axis=0))[:last_index])\n",
    "    diffs_squared = np.array(diffs)**2\n",
    "    E_diffs_sqared = np.mean(diffs_squared, axis=0)\n",
    "    preds_squared = np.array(preds)**2\n",
    "    E_preds_squared = np.mean(preds_squared, axis=0)\n",
    "    return  pd.DataFrame({\"time\": all_explanation.timestamps[:last_index], \"sigma\": np.sqrt(E_diffs_sqared) / np.sqrt(E_preds_squared), \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_accuracy_rsf = get_local_accuracy_from_shap_explanations(exp1_survshap_global_rsf, \"RSF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_accuracy_cph = get_local_accuracy_from_shap_explanations(exp1_survshap_global_cph, \"CPH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([local_accuracy_rsf, local_accuracy_cph]).to_csv(\"results/exp1_local_accuracy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GT-Shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_grountruth_rsf = shap_grountruth_rsf.sort_values(by=[\"observation_index\", \"variable_index\"]) \n",
    "shap_grountruth_cph = shap_grountruth_cph.sort_values(by=[\"observation_index\", \"variable_index\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_rsf = shap_grountruth_rsf.values[:, 2:] - exp1_survshap_global_rsf.full_result[exp1_survshap_global_rsf.full_result[\"index\"].isin(shap_grountruth_rsf[\"observation_index\"])].values[:, 6:]\n",
    "diff_cph = shap_grountruth_cph.values[:, 2:] - exp1_survshap_global_cph.full_result[exp1_survshap_global_cph.full_result[\"index\"].isin(shap_grountruth_cph[\"observation_index\"])].values[:, 6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_shap_profiles_rsf = shap_grountruth_rsf.values[:, 2:]\n",
    "gt_shap_profiles_cph = shap_grountruth_cph.values[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_rsf = np.sqrt(np.array(np.mean(diff_rsf**2, axis=0), dtype=np.float64))\n",
    "rmse_gt_shap_profiles_rsf = np.sqrt(np.array(np.mean(gt_shap_profiles_rsf**2, axis=0), dtype=np.float64))\n",
    "rmse_cph = np.sqrt(np.array(np.mean(diff_cph**2, axis=0), dtype=np.float64))\n",
    "rmse_gt_shap_profiles_cph = np.sqrt(np.array(np.mean(gt_shap_profiles_cph**2, axis=0), dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_comp_by_vars_rsf = np.zeros((5, 669))\n",
    "for i in range(5): \n",
    "    rmse = np.sqrt(np.array(np.mean(diff_rsf[i::5,] **2, axis=0), dtype=np.float64)) \n",
    "    normalization_factor = np.sqrt(np.array(np.mean(gt_shap_profiles_rsf[i::5,]**2, axis=0), dtype=np.float64))\n",
    "    gt_comp_by_vars_rsf[i,:] = rmse/normalization_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_comp_by_vars_cph = np.zeros((5, 1000))\n",
    "for i in range(5): \n",
    "    rmse = np.sqrt(np.array(np.mean(diff_cph[i::5,] **2, axis=0), dtype=np.float64)) \n",
    "    normalization_factor = np.sqrt(np.array(np.mean(gt_shap_profiles_cph[i::5,]**2, axis=0), dtype=np.float64))\n",
    "    gt_comp_by_vars_cph[i,:] = rmse/normalization_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(gt_comp_by_vars_rsf, index=[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"], columns=timestamps_rsf).reset_index().rename(columns={\"index\": \"variable_name\"})\n",
    "pd.melt(tmp, id_vars=\"variable_name\", value_vars=tmp.columns).to_csv(\"results/exp1_gt_shap_rsf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(gt_comp_by_vars_cph, index=[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"], columns=timestamps_cph).reset_index().rename(columns={\"index\": \"variable_name\"})\n",
    "pd.melt(tmp, id_vars=\"variable_name\", value_vars=tmp.columns).to_csv(\"results/exp1_gt_shap_cph.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GT_Shapley(all_explanation, groundtruth, label):\n",
    "    corrs = []\n",
    "    for i in range(1, 100):\n",
    "        gt = (groundtruth.values[(i*5):(i*5+5), 2:])\n",
    "        obt = (np.array(all_explanation.full_result[all_explanation.full_result[\"index\"].isin(groundtruth[\"observation_index\"])].values[(i*5):(i*5+5), 6:], dtype=np.float64))\n",
    "        corrs.append(pd.DataFrame(gt).corrwith(pd.DataFrame(obt), axis=0))\n",
    "    return  pd.DataFrame({\"time\": all_explanation.timestamps, \"correlation\": np.array(corrs).mean(axis=0), \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([GT_Shapley(exp1_survshap_global_rsf, shap_grountruth_rsf, \"RSF\"), GT_Shapley(exp1_survshap_global_cph, shap_grountruth_cph, \"CPH\")]).to_csv(\"results/exp1_corr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('survshap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e89d17bf3f53615b213f4c00662e1677a8885f31ece09e136535e9b43ddada0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
